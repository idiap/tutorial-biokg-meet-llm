{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dspy"
      ],
      "metadata": {
        "id": "pYsbn3SdLMX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "o43F8vIo0ISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzr9Z7cbrGz2"
      },
      "outputs": [],
      "source": [
        "TITLE_MARKER = \"|t|\"\n",
        "ABSTRACT_MARKER = \"|a|\"\n",
        "CID_MARKER = \"CID\"\n",
        "\n",
        "\n",
        "def remove_duplicates(dict_list):\n",
        "    \"\"\"\n",
        "    Remove duplicate dictionaries from a list.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    unique_dicts = []\n",
        "    for d in dict_list:\n",
        "        # If several entities have been extracted - we are just going to keep the first span\n",
        "        sub_d = {\"text\": d[\"text\"], \"type\": d[\"type\"], \"identifier\": d[\"identifier\"]}\n",
        "        key = frozenset(sub_d.items())\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_dicts.append(d)\n",
        "    return unique_dicts\n",
        "\n",
        "def parse_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Parse the BioCreative dataset into structured dictionaries.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the dataset file.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of documents, each with id, title, abstract, and annotations.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    current_doc = None\n",
        "\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for raw_line in f:\n",
        "            line = raw_line.strip()\n",
        "            if not line:\n",
        "                continue  # skip blank lines\n",
        "\n",
        "            if TITLE_MARKER in line:\n",
        "                # Save the previous document\n",
        "                if current_doc:\n",
        "                    documents.append(current_doc)\n",
        "\n",
        "                doc_id, _, title = line.partition(TITLE_MARKER)\n",
        "                current_doc = {\"id\": doc_id, \"text\": title, \"annotations\": []}\n",
        "\n",
        "            elif ABSTRACT_MARKER in line:\n",
        "                _, _, abstract = line.partition(ABSTRACT_MARKER)\n",
        "                if current_doc is not None:\n",
        "                    current_doc[\"text\"] += \"\\n\" + abstract\n",
        "\n",
        "            else:\n",
        "                # Annotation or CID line\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) < 6:\n",
        "                    continue  # malformed line\n",
        "\n",
        "                if parts[1] == CID_MARKER:\n",
        "                    continue  # skip chemical-disease relation lines\n",
        "\n",
        "                _, span_start, span_end, text, entity_type, identifier = parts[:6]\n",
        "                current_doc[\"annotations\"].append({\n",
        "                    \"text\": text,\n",
        "                    \"type\": entity_type,\n",
        "                    \"identifier\": identifier\n",
        "                })\n",
        "\n",
        "        # Add last document\n",
        "        if current_doc:\n",
        "            documents.append(current_doc)\n",
        "\n",
        "    # Deduplicate annotations\n",
        "    for doc in documents:\n",
        "        doc[\"annotations\"] = remove_duplicates(doc[\"annotations\"])\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/CDR_TestSet.PubTator.txt\"\n",
        "dataset = parse_dataset(dataset_path)"
      ],
      "metadata": {
        "id": "yYRD-eYdwmU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_kb(file_path):\n",
        "    \"\"\"\n",
        "    Load KB from a JSONL file into alias list and concept mapping.\n",
        "    \"\"\"\n",
        "    aliases = []\n",
        "    alias_to_concept = []\n",
        "    concept_meta = {}\n",
        "\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            concept = json.loads(line)\n",
        "            cid = concept[\"concept_id\"]\n",
        "            concept_meta[cid] = concept\n",
        "\n",
        "            for alias in concept[\"aliases\"]:\n",
        "                aliases.append(alias)\n",
        "                alias_to_concept.append(cid)\n",
        "\n",
        "    return aliases, alias_to_concept, concept_meta\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Lowercase, remove punctuation for consistency.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def build_tfidf_index(aliases):\n",
        "    vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5))\n",
        "    alias_vecs = vectorizer.fit_transform(preprocess(a) for a in aliases)\n",
        "    # Normalize for cosine similarity\n",
        "    alias_vecs = normalize(alias_vecs)\n",
        "    return vectorizer, alias_vecs\n",
        "\n",
        "def batch_link_entities(mentions, vectorizer, alias_vecs, aliases, alias_to_concept, top_k=5):\n",
        "    mention_vecs = vectorizer.transform(preprocess(m) for m in mentions)\n",
        "    mention_vecs = normalize(mention_vecs)\n",
        "    sims = mention_vecs @ alias_vecs.T  # Matrix multiplication, very fast\n",
        "\n",
        "    results_batch = []\n",
        "    for i, mention in enumerate(mentions):\n",
        "        row = sims[i].toarray().ravel()\n",
        "        top_idx = np.argsort(-row)[:top_k]\n",
        "        results = []\n",
        "        for idx in top_idx:\n",
        "            results.append({\n",
        "                \"mention\": mention,\n",
        "                \"alias\": aliases[idx],\n",
        "                \"concept_id\": alias_to_concept[idx],\n",
        "                \"score\": float(row[idx])\n",
        "            })\n",
        "        results_batch.append(results)\n",
        "    return results_batch\n",
        "\n",
        "\n",
        "kb_file = \"/content/mesh_2020.jsonl\"\n",
        "\n",
        "print(\"Loading KB...\")\n",
        "aliases, alias_to_concept, concept_meta = load_kb(kb_file)\n",
        "print(f\"Loaded {len(aliases)} aliases from KB.\")\n",
        "\n",
        "print(\"Building TF-IDF index...\")\n",
        "vectorizer, alias_vecs = build_tfidf_index(aliases)\n",
        "\n",
        "# here an example\n",
        "mentions = [\"delirium\", \"hypertension\"]\n",
        "mention_results = batch_link_entities(mentions, vectorizer, alias_vecs, aliases, alias_to_concept, top_k=5)\n",
        "\n",
        "for mention_results in mention_results:\n",
        "    print(f\"\\nMention: {mention_results[0]['mention']}\")\n",
        "    for r in mention_results:\n",
        "        meta = concept_meta[r[\"concept_id\"]]\n",
        "        print(f\"  {r['alias']} → {r['concept_id']} ({meta['canonical_name']}) score={r['score']:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fNJNCjVLwvYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_K=5"
      ],
      "metadata": {
        "id": "m9DUr0tb1wWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "display_error = True\n",
        "TOTAL_FOUND = 0\n",
        "TOTAL = 0\n",
        "# We only process the first 100 articles.\n",
        "\n",
        "for item in tqdm(dataset[:100], desc=\"process all the dataset\"):\n",
        "\n",
        "    mentions = [annotation[\"text\"] for annotation in item[\"annotations\"]]\n",
        "    ground_truths = [annotation[\"identifier\"] for annotation in item[\"annotations\"]]\n",
        "    results = batch_link_entities(mentions, vectorizer, alias_vecs, aliases, alias_to_concept, top_k=TOP_K)\n",
        "    for i, (result, ground_truth) in enumerate(zip(results, ground_truths)):\n",
        "        if ground_truth in [entity[\"concept_id\"] for entity in result]:\n",
        "            TOTAL_FOUND += 1\n",
        "        else:\n",
        "            if display_error:\n",
        "                matched = ', '.join([f\"{entity['alias']} ({entity['concept_id']})\" for entity in result])\n",
        "                print(f\"{mentions[i]}: matched {matched}).\\nGround truth was {ground_truth}\\n\")\n",
        "        TOTAL += 1\n",
        "print(f\"Recall @{TOP_K} {(TOTAL_FOUND/TOTAL) * 100}\")"
      ],
      "metadata": {
        "id": "Yh0n8WoTBpRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Recall @{TOP_K} {(TOTAL_FOUND/TOTAL) * 100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP8rbXvRCaK9",
        "outputId": "b4d97cbc-c37f-4b20-bf41-72f67465557b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall @5 72.00424178154825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLM"
      ],
      "metadata": {
        "id": "D_790TsMEi5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "from typing import List\n",
        "\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class LLMConfig:\n",
        "    @staticmethod\n",
        "    def setup_openai(api_key: str, model: str = \"gpt-4o-mini\", max_tokens=2048):\n",
        "        \"\"\"Setup OpenAI API\"\"\"\n",
        "        import openai\n",
        "        openai.api_key = api_key\n",
        "        lm = dspy.LM(model, max_tokens=max_tokens, api_key=api_key)\n",
        "        dspy.configure(lm=lm, adapter=dspy.JSONAdapter())\n",
        "        dspy.settings.lm.kwargs[\"temperature\"] = 0.0\n",
        "        return lm\n",
        "\n",
        "    @staticmethod\n",
        "    def setup_ollama(api_base: str = \"http://127.0.0.1:11434\", model: str = \"llama3.1:8b\", max_tokens=2048):\n",
        "        \"\"\"Setup Ollama local model\"\"\"\n",
        "        lm = dspy.LM(model, api_base=api_base, max_tokens=max_tokens)\n",
        "        dspy.configure(lm=lm, adapter=dspy.JSONAdapter())\n",
        "        dspy.settings.lm.kwargs[\"temperature\"] = 0.0\n",
        "        return lm\n",
        "\n",
        "class NamedEntity(BaseModel):\n",
        "    text: str\n",
        "    type: str\n",
        "\n",
        "class EntityLinkingInput(BaseModel):\n",
        "    text: str\n",
        "    entities: List[NamedEntity]\n",
        "\n",
        "class LinkedEntity(BaseModel):\n",
        "    text: str\n",
        "    mesh_id: str\n",
        "\n",
        "\n",
        "class EntityLinkingSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Given a scientific text and a list of extracted named entities,\n",
        "    Link each entity to the MeSH ID from the Medical Subject Headings (MeSH) thesaurus.\n",
        "    \"\"\"\n",
        "    text: str = dspy.InputField(desc=\"The full document text\")\n",
        "    entities: EntityLinkingInput = dspy.InputField(desc=\"List of extracted entities, each with text and type\")\n",
        "    linked_entities: List[LinkedEntity] = dspy.OutputField(desc=\"List of the prediceted MeSH ID\")\n",
        "\n",
        "class EntityLinkerModule(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linker_llm = dspy.Predict(EntityLinkingSignature)\n",
        "\n",
        "    def forward(self, item):\n",
        "        text = item[\"text\"]\n",
        "        entities = [{\"text\": annot.get('text', ''), \"type\": annot.get('type', '')} for annot in item['annotations']]\n",
        "        return self.linker_llm(text=text, entities=entities)\n"
      ],
      "metadata": {
        "id": "vxBGsrgAEuof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt update && sudo apt install pciutils lshw\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "Tk2mevoBKSaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > ollama.log 2>&1 &"
      ],
      "metadata": {
        "id": "y0lljyWgKS4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run vicuna:13b “What is the capital of the Netherlands?”"
      ],
      "metadata": {
        "id": "pNBn32PqKeQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "# load .env vars\n",
        "_ = load_dotenv(find_dotenv())\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "llm = LLMConfig.setup_openai(model=\"gpt-4.1-mini\", api_key=api_key)\n",
        "# llm = LLMConfig.setup_ollama(model=\"ollama_chat/llama3.1:8b\", api_base=\"http://127.0.0.1:11434\")"
      ],
      "metadata": {
        "id": "ykPHI0OZHQbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just a test !\n",
        "llm(\"Hello !\")"
      ],
      "metadata": {
        "id": "8KMUIbOqIb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ntu0nn9HrxTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLMLinker = EntityLinkerModule()"
      ],
      "metadata": {
        "id": "eyFDA_svHeSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "TOTAL_FOUND = 0\n",
        "TOTAL = 0\n",
        "# We only process the first 100 articles.\n",
        "all_responses = {}\n",
        "\n",
        "for item in tqdm(dataset[:100], desc=\"process all the dataset\"):\n",
        "    ground_truth = {}\n",
        "    for annot in item[\"annotations\"]:\n",
        "        ground_truth[annot[\"text\"]] = annot[\"identifier\"]\n",
        "\n",
        "    # Compute response\n",
        "    response = LLMLinker(item)\n",
        "    predicted = {}\n",
        "    all_responses[item['id']] = ([le.model_dump() for le in response.linked_entities])\n",
        "    for linked_ent in response.linked_entities:\n",
        "        predicted[linked_ent.text] = linked_ent.mesh_id\n",
        "\n",
        "    # Build a dict for the groudn truth and the response outputs\n",
        "    for text, gt_mesh_id in ground_truth.items():\n",
        "        if predicted.get(text, '') == gt_mesh_id:\n",
        "          TOTAL_FOUND += 1\n",
        "        TOTAL += 1\n"
      ],
      "metadata": {
        "id": "7bKXvUL0Hh-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LLM - Recall @1 {(TOTAL_FOUND/TOTAL) * 100}\")"
      ],
      "metadata": {
        "id": "athd0cnBsuF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the response in a cache file\n",
        "with open(\"all_gpt-4.1-mini_responses.json\", \"w\") as f:\n",
        "    json.dump(all_responses, f, indent=4)"
      ],
      "metadata": {
        "id": "G1hlsW-fsmIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Cache\n",
        "cached_response = {}\n",
        "with open(\"all_gpt-4.1-mini_responses.json\", \"r\") as f:\n",
        "    cached_response = json.load(f)\n",
        "\n",
        "\n",
        "TOTAL_FOUND = 0\n",
        "TOTAL = 0\n",
        "\n",
        "for item in tqdm(dataset[:100], desc=\"process all the dataset\"):\n",
        "    ground_truth = {}\n",
        "    for annot in item[\"annotations\"]:\n",
        "        ground_truth[annot[\"text\"]] = annot[\"identifier\"]\n",
        "\n",
        "    # Compute response\n",
        "    response = [LinkedEntity(**entity) for entity in cached_response[item['id']]]\n",
        "    predicted = {}\n",
        "    for linked_ent in response:\n",
        "        predicted[linked_ent.text] = linked_ent.mesh_id\n",
        "\n",
        "    # Build a dict for the groudn truth and the response outputs\n",
        "    for text, gt_mesh_id in ground_truth.items():\n",
        "        if predicted.get(text, '') == gt_mesh_id:\n",
        "          TOTAL_FOUND += 1\n",
        "        TOTAL += 1\n"
      ],
      "metadata": {
        "id": "_R2xuinErzSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LLM (from cache) - Recall @1 {(TOTAL_FOUND/TOTAL) * 100}\")"
      ],
      "metadata": {
        "id": "3AoWyAlKNNeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is way slower, and obviously: way worse !\n",
        "\n",
        "Maybe not for linking ... but how could they help ?\n",
        "\n",
        "[*LLM as Entity Disambiguator for Biomedical Entity-Linking*](https://doi.org/10.18653/v1/2025.acl-short.25)\n",
        "\n",
        "[*Retrieve Then Rerank: An End-to-End Learning Paradigm for Biomedical Entity Linking*](https://doi.org/10.1111/jebm.70053)\n",
        "\n",
        "[*BioLinkerAI: Leveraging LLMs to Improve Biomedical Entity Linking and Knowledge Capture*](https://doi.org/10.1145/3701551.3708812)\n",
        "\n",
        "[*Improving biomedical entity linking for complex entity mentions with LLM-based text simplification*](https://doi.org/10.1093/database/baae067)\n",
        "\n",
        "[*Accelerating Cross-Encoders in Biomedical Entity Linking*](10.18653/v1/2025.bionlp-1.13)\n",
        "\n",
        "[*Contextual Augmentation for Entity Linking using Large Language Models*](https://aclanthology.org/2025.coling-main.570/)"
      ],
      "metadata": {
        "id": "4kPSJl0jPeTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can an LLM help to filter the initial candidates list ?"
      ],
      "metadata": {
        "id": "GZzjI690R87s"
      }
    }
  ]
}